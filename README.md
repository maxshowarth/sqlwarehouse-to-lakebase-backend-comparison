# sqlwarehouse-to-lakebase-backend-comparison

This project is an experimental **Databricks Asset Bundle (DAB)** application that demonstrates how the **same Streamlit UI** can run against two different backends:

- **Databricks SQL Warehouse** (OLAP, analytical aggregations)
- **Lakehouse/Lakebase** (OLTP, low-latency point queries and updates)

The end goal is to let users **toggle the backend** in the app and compare performance and behavior directly.

For now, the app reads **local CSVs** generated by our seeding script. These CSVs act as a stand-in for future backends.

---


## Generating Sample Data

The `seed_data.py` script produces realistic fake retail data inspired by the [Brickhouse Brands demo](https://github.com/databricks-solutions/brickhouse-brands-demo).
It creates multiple CSVs in `sample_data/`, including stores, products, customers, orders, order_items, inventory snapshots, and promotions.

### Example

```bash
# From repo root
uv run app/backend/seed_data.py --scale small --days 14
```

Output looks like:

```
Generated data in sample_data
 stores: 10 | products: 200 | customers: 2000
 orders: 3966 | order_items: 11190 | promotions: 61
 inventory_snapshots: 28000
```

Scales available: `small`, `medium`, `large`.

---

## Running the App Locally

The Streamlit app reads directly from CSV files. You can either use the generated sample data or point to your own data directory.

### Using Sample Data

Generate sample data first:

```bash
# From repo root
python app/backend/seed_data.py --scale small --days 14
```

Then run the app:

```bash
databricks apps run-local --prepare-environment --debug
```

### Using Your Own Data Directory

You can specify a custom data directory using the `DATA_DIR` environment variable:

#### Option 1: Environment Variable
```bash
export DATA_DIR="/path/to/your/data"
databricks apps run-local --prepare-environment --debug
```

#### Option 2: .env File
Create a `.env` file in the repository root:
```env
DATA_DIR=/path/to/your/data
```

#### Option 3: Relative Path
If your data is in a subdirectory of the repository:
```bash
export DATA_DIR="my_data_folder"
```

### Required CSV Files

Your data directory must contain these CSV files:
- `orders.csv` - Order records with columns: order_id, store_id, customer_id, order_ts, payment_type, discount_pct
- `order_items.csv` - Order line items with columns: order_id, line_number, product_id, qty, unit_price, extended_price
- `products.csv` - Product catalog with columns: product_id, sku, name, category, brand, base_price
- `stores.csv` - Store locations with columns: store_id, name, region, city, latitude, longitude, opened_date

Open the provided local URL in your browser.

---

## Deploying as a DAB App

The bundle currently deploys a **hello-world Streamlit app** to Databricks.

```bash
databricks bundle deploy -t dev
```

In the Databricks workspace, navigate to **Apps** and open the deployed Streamlit app.

---

## Notes

- The current state is **CSV-only**: all data comes from local files.
- CSVs are excluded from version control; run the seed script whenever you need fresh data.
- The app and seed script are intentionally **minimal** so we can iterate quickly before wiring up the real backends.
