# sqlwarehouse-to-lakebase-backend-comparison

This project is an experimental **Databricks Asset Bundle (DAB)** application that demonstrates how the **same Streamlit UI** can run against two different backends:

- **Databricks SQL Warehouse** (OLAP, analytical aggregations)
- **Lakehouse/Lakebase** (OLTP, low-latency point queries and updates)

The end goal is to let users **toggle the backend** in the app and compare performance and behavior directly.

For now, the app reads **local CSVs** generated by our seeding script. These CSVs act as a stand-in for future backends.

---

## Generating Sample Data

### Local Development

The `generate_data.py` script in the project root provides command-line access to the data generation functions. It creates realistic fake retail data inspired by the [Brickhouse Brands demo](https://github.com/databricks-solutions/brickhouse-brands-demo).
It creates multiple CSVs in your specified output directory, including stores, products, customers, orders, order_items, inventory snapshots, and promotions.

#### Example

```bash
# From repo root
python generate_data.py --scale small --days 14 --output-dir sample_data

# Or with UV
uv run generate_data.py --scale small --days 14 --output-dir sample_data
```

Output looks like:

```
Generated data in sample_data
 stores: 10 | products: 200 | customers: 2000
 orders: 3966 | order_items: 11190 | promotions: 61
 inventory_snapshots: 28000
```

Scales available: `small`, `medium`, `large`.

### Databricks Pipeline

For production data generation, the project includes a **Databricks Asset Bundle pipeline** that generates the same synthetic data directly in your Lakehouse catalog.

#### Quick Start

```bash
# Deploy and run with default settings
cd pipelines/data_generation
./deploy.sh dev

# Custom parameters
./deploy.sh dev medium 30 false
```

#### Features

- **Complete Dataset**: Generates all 7 tables with proper referential integrity
- **Configurable Scale**: Choose from small, medium, or large data volumes
- **Overwrite Control**: Flag to determine whether to overwrite existing data
- **Fail-Fast**: Pipeline fails completely if any table generation fails
- **Single Configuration**: Uses the same catalog/schema as your main DAB

#### Configuration

The pipeline uses the same catalog and schema configuration as defined in your main `databricks.yml`:

```yaml
variables:
  catalog: max_howarth_demos
  schema: sqlw-to-lakebase-backend
```

See [pipelines/data_generation/README.md](pipelines/data_generation/README.md) for detailed documentation.

---

## Running the App Locally

The Streamlit app reads directly from CSV files. You can either use the generated sample data or point to your own data directory.

### Using Sample Data

Generate sample data first:

```bash
# From repo root
python generate_data.py --scale small --days 14 --output-dir sample_data
```

Then run the app:

```bash
databricks apps run-local --prepare-environment --debug
```

### Using Your Own Data Directory

You can specify a custom data directory using the `DATA_DIR` environment variable:

#### Option 1: Environment Variable
```bash
export DATA_DIR="/path/to/your/data"
databricks apps run-local --prepare-environment --debug
```

#### Option 2: .env File
Create a `.env` file in the repository root:
```env
DATA_DIR=/path/to/your/data
```

#### Option 3: Relative Path
If your data is in a subdirectory of the repository:
```bash
export DATA_DIR="my_data_folder"
```

### Required CSV Files

Your data directory must contain these CSV files:
- `orders.csv` - Order records with columns: order_id, store_id, customer_id, order_ts, payment_type, discount_pct
- `order_items.csv` - Order line items with columns: order_id, line_number, product_id, qty, unit_price, extended_price
- `products.csv` - Product catalog with columns: product_id, sku, name, category, brand, base_price
- `stores.csv` - Store locations with columns: store_id, name, region, city, latitude, longitude, opened_date

Open the provided local URL in your browser.

---

## Deploying as a DAB App

The bundle currently deploys a **hello-world Streamlit app** to Databricks.

```bash
databricks bundle deploy -t dev
```

## Project Structure

```
sqlwarehouse-to-lakebase-backend-comparison/
├── databricks.yml                    # Main bundle configuration
├── generate_data.py                  # CLI wrapper script
├── app/                              # Application code
├── pipelines/
│   └── data_generation/              # Pipeline implementation
│       ├── data_generators.py        # Data generation functions
│       ├── generate_data_job.py      # Databricks job script
│       ├── config.json               # Configuration options
│       ├── deploy.sh                 # Deployment script
│       └── README.md                 # Pipeline documentation
└── ...
```

The CLI wrapper in the root provides easy access to the data generation functions, while the actual implementation and pipeline are organized in the `pipelines/data_generation/` folder.
