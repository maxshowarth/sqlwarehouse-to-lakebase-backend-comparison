# sqlwarehouse-to-lakebase-backend-comparison

This project is an experimental **Databricks Asset Bundle (DAB)** application that demonstrates how the **same Streamlit UI** can run against two different backends:

- **Databricks SQL Warehouse** (OLAP, analytical aggregations)
- **Lakehouse/Lakebase** (OLTP, low-latency point queries and updates)

The end goal is to let users **toggle the backend** in the app and compare performance and behavior directly.

For now, the app reads **local CSVs** generated by our seeding script. These CSVs act as a stand-in for future backends.

---


## Generating Sample Data

The `seed_data.py` script produces realistic fake retail data inspired by the [Brickhouse Brands demo](https://github.com/databricks-solutions/brickhouse-brands-demo).
It creates multiple CSVs in `sample_data/`, including stores, products, customers, orders, order_items, inventory snapshots, and promotions.

### Example

```bash
# From repo root
uv run src/seed/seed_data.py --scale small --days 14
```

Output looks like:

```
Generated data in sample_data
 stores: 10 | products: 200 | customers: 2000
 orders: 3966 | order_items: 11190 | promotions: 61
 inventory_snapshots: 28000
```

Scales available: `small`, `medium`, `large`.

---

## Running the App Locally

The Streamlit app reads directly from the CSVs under `sample_data/`.

```bash
databricks apps run-local --prepare-environment --debug
```

Open the provided local URL in your browser.

---

## Deploying as a DAB App

The bundle currently deploys a **hello-world Streamlit app** to Databricks.

```bash
databricks bundle deploy -t dev
```

In the Databricks workspace, navigate to **Apps** and open the deployed Streamlit app.

---

## Notes

- The current state is **CSV-only**: all data comes from local files.
- CSVs are excluded from version control; run the seed script whenever you need fresh data.
- The app and seed script are intentionally **minimal** so we can iterate quickly before wiring up the real backends.
